{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Reinforcement learning model notebook\n",
                "\n",
                "This notebook will allow you to run the reinforcement learning model presented in the paper on an example participant, loading in their specific subject csv\n",
                "\n",
                "The code for the reinforcement learning model is found under src.rl.mb_mf_fit\n",
                "\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\n",
                "from src.rl.mb_mf_fit import MB_MF_rllik_learn_mat_arms, param_init, apply_priors\n",
                "import scipy.optimize\n",
                "import numpy as np"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# setting parameters (note this is not taking into account the priors)\n",
                "bounds = [\n",
                "                (0.00001, 20),\n",
                "                (0.00001, 0.9999),\n",
                "                (0.00001, 0.9999),\n",
                "                (0.00001, 0.9999),\n",
                "                (0.00001, 0.9999),\n",
                "                (0.00001, 0.9999),\n",
                "                (0.00001, 0.9999),\n",
                "                (-20, 20),\n",
                "                (-20, 20),\n",
                "                (0.00001, 0.9999),\n",
                "                (0.00001, 0.9999),\n",
                "            ]\n",
                "def param_init(bounds):\n",
                "    \"\"\"\n",
                "    feed in a list of tuples of bounds and the param_init function will return a set of starting point params from within those bounds\n",
                "    \"\"\"\n",
                "    params = []\n",
                "    for i in range(len(bounds)):\n",
                "        lower = bounds[i][0]\n",
                "        higher = bounds[i][1]\n",
                "        curr_param = np.random.default_rng(202204).uniform(low=lower, high=higher, size=1)\n",
                "        params.append(curr_param[0])\n",
                "    return params\n",
                "params =param_init(bounds)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "sub_df = pd.read_csv('../../data/interim/experiment_2/subject_csvs/sub_A1CUDX7TTS2W61.csv')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The following runs through a single iteration of the reinforcement learning model, with a single $w$ parameter, a learning of the transition matrix, as well as $\\eta$ and $\\kappa$ each being left as free parameters this function will return a negative log-likelihood value for this iteration but the priors have not been applied here"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# single run \n",
                "MB_MF_rllik_learn_mat_arms(\n",
                "    params, sub_df, stakes='1', final=False, kappa_equivalent=False\n",
                ")"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "array([977.33081467])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now we will attach the code that weights this negative log-likelihood by our priors and we will get a slightly different value"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "model_func_parameters = [sub_df, '1', False, False]\n",
                "apply_priors(params, model_func_parameters)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "array([1019.15892043])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 5
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now we can use the full scipy.optimize.minimize function to find the optimal parameters for this individual given our priors (this will take approximately 30 seconds)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "fit = scipy.optimize.minimize(\n",
                "    apply_priors,\n",
                "    params,\n",
                "    args=([sub_df, '1', False, False]),\n",
                "    method=\"L-BFGS-B\",\n",
                "    bounds=bounds,\n",
                ")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "fit"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "      fun: array([154.58468828])\n",
                            " hess_inv: <11x11 LbfgsInvHessProduct with dtype=float64>\n",
                            "      jac: array([ 0.00028706,  0.00090949,  0.00017337,  0.00035811,  0.00013927,\n",
                            "        0.00013927,  0.00013927, -0.00093507,  0.00253522,  0.00043769,\n",
                            "       -0.00021032])\n",
                            "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
                            "     nfev: 864\n",
                            "      nit: 52\n",
                            "     njev: 72\n",
                            "   status: 0\n",
                            "  success: True\n",
                            "        x: array([ 2.19670665,  0.81635205,  0.52280367,  0.88147683,  0.50001739,\n",
                            "        0.50001739,  0.50001739,  0.05185304, -0.14285279,  0.42637492,\n",
                            "        0.46106694])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 7
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Here we can see the final negative log likelihood was 154.584\n",
                "The number of iterations was 39\n",
                "and we get values out for each of the parameters. \n",
                "\n",
                "This subject's   \n",
                "softmax inverse temp $\\beta$ : 2.19670665  \n",
                "learning rate $\\alpha$ : 0.81635205     \n",
                "eligibility trace decay $\\lambda$ : 0.52280367     \n",
                "weight low stakes low arm $w$ : 0.88147683 (this will be the overall weight if the stakes flag is not triggered)  \n",
                "weight high stakes low arm (we fit with stakes = '1' so this isn't fit away from the initialization)  \n",
                "weight low stakes high arm (we fit with stakes = '1' so this isn't fit away from the initialization)  \n",
                "weight high stakes high arm (we fit with stakes = '1' so this isn't fit away from the initialization)  \n",
                "stickiness $\\pi$ : 0.05185304    \n",
                "response stickiness $\\rho$ : -0.14285279     \n",
                "transition matrix updating $\\eta$ : 0.42637492      \n",
                "sophisticated updating of the other action $\\kappa$ : 0.46106694    "
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.12",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.12 64-bit ('mb-cog-maps-paper': conda)"
        },
        "interpreter": {
            "hash": "46a92a374ab22580d330e785dc2d7bbc28533baedc701d1d9ab6ff664babcd13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}